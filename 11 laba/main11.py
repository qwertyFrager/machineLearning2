import re
import string
import pandas as pd
from sklearn.feature_extraction.text import CountVectorizer  # Bag of Words
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import precision_score, recall_score, f1_score
from sklearn.model_selection import train_test_split
from nltk.stem import WordNetLemmatizer
from nltk.corpus import stopwords
import nltk

# Загрузка данных
data = pd.read_csv('DataSet13_1.csv', sep=',')
data.head()

# Разделение данных на обучающую и тестовую выборки (без предобработки)
X_train_raw, X_test_raw, y_train_raw, y_test_raw = train_test_split(
    data['text'], data['author'], test_size=0.2, random_state=42
)

# Bag of Words (без предобработки)
vectorizer_raw = CountVectorizer()
X_train_raw = vectorizer_raw.fit_transform(X_train_raw)
X_test_raw = vectorizer_raw.transform(X_test_raw)

# Обучение классификатора (без предобработки)
classifier_raw = LogisticRegression(max_iter=1000)
classifier_raw.fit(X_train_raw, y_train_raw)

# Предсказание и оценка (без предобработки)
y_pred_raw = classifier_raw.predict(X_test_raw)

# Метрики качества (без предобработки)
precision_raw = precision_score(y_test_raw, y_pred_raw, average='weighted')
recall_raw = recall_score(y_test_raw, y_pred_raw, average='weighted')
f1_raw = f1_score(y_test_raw, y_pred_raw, average='weighted')

print("Результаты классификации без предобработки:")
print(f'Precision BoW Score = {precision_raw}')
print(f'Recall BoW Score = {recall_raw}')
print(f'F1 BoW Score = {f1_raw}')

# До предобработки модель работает относительно неплохо. Однако из-за наличия "шума" (стоп-слов, пунктуации и пр.) и
# отсутствия унификации (например, разных форм одного слова) модель может путаться в важных и незначительных
# характеристиках текста. Это приводит к тому, что:
#
# Некоторые признаки, такие как пунктуация и часто встречающиеся слова, не несут информации для классификации, но
# учитываются моделью.
# Слова с похожим значением (например, "работать" и "работает") обрабатываются как разные признаки.
# Эти факторы снижают общую эффективность модели, что отражается на метриках.



# Предобработка текста
nltk.download('wordnet')
nltk.download('stopwords')

lemmatizer = WordNetLemmatizer()
STOPWORDS = set(stopwords.words('russian'))
PUNCT_TO_REMOVE = string.punctuation

def remove_punctuation(text):
    """Удаление пунктуации."""
    return text.translate(str.maketrans('', '', PUNCT_TO_REMOVE))

def remove_stopwords(text):
    """Удаление стоп-слов."""
    return " ".join([word for word in text.split() if word not in STOPWORDS])

def lemmatize_words(text):
    """Лемматизация слов."""
    return " ".join([lemmatizer.lemmatize(word) for word in text.split()])

def preprocess_text(text):
    """Полная предобработка текста."""
    text = remove_punctuation(text)
    text = remove_stopwords(text)
    text = lemmatize_words(text)
    return text

# Применение предобработки
data['text_preprocessed'] = data['text'].apply(preprocess_text)

# Разделение данных на обучающую и тестовую выборки (с предобработкой)
X_train, X_test, y_train, y_test = train_test_split(
    data['text_preprocessed'], data['author'], test_size=0.2, random_state=42
)

# Bag of Words (с предобработкой)
vectorizer = CountVectorizer()
X_train = vectorizer.fit_transform(X_train)
X_test = vectorizer.transform(X_test)

# Обучение классификатора (с предобработкой)
classifier = LogisticRegression(max_iter=1000)
classifier.fit(X_train, y_train)

# Предсказание и оценка (с предобработкой)
y_pred = classifier.predict(X_test)

# Метрики качества (с предобработкой)
precision = precision_score(y_test, y_pred, average='weighted')
recall = recall_score(y_test, y_pred, average='weighted')
f1 = f1_score(y_test, y_pred, average='weighted')

print("\nРезультаты классификации с предобработкой:")
print(f'Precision BoW Score = {precision}')
print(f'Recall BoW Score = {recall}')
print(f'F1 BoW Score = {f1}')

# Precision: Увеличение с 0.7745 до 0.8155 означает, что после предобработки модель делает меньше ложных положительных
# предсказаний (то есть, лучше различает классы).
# Recall: Увеличение с 0.7810 до 0.8127 говорит о том, что модель охватывает больше примеров каждого класса.
# F1 Score: Рост с 0.7690 до 0.7976 показывает общее улучшение баланса между точностью и полнотой, что особенно важно
# для несбалансированных данных.


# Токенизация
# Что это?
#
# Процесс разбиения текста на отдельные слова (токены) или другие минимальные единицы, такие как фразы или символы.
# Пример: Текст: "Машинное обучение — это круто!"
# Токены: ["Машинное", "обучение", "это", "круто"]

# Зачем это нужно?
# Чтобы разбить текст на составляющие для анализа модели, так как модели работают с токенами, а не с текстом целиком.


# Удаление стоп-слов (шумовых слов)
# Что это?
#
# Удаление часто встречающихся, но несущих малую смысловую нагрузку слов (например, "и", "это", "в").
# Пример: Текст: "Это важно в машинном обучении"
# Без стоп-слов: "важно машинном обучении"

# Зачем это нужно?
# Чтобы уменьшить "шум" в данных и позволить модели сосредоточиться на значимых признаках.


# Удаление специальных символов
# Что это?
#
# Удаление пунктуации, цифр и других символов, которые не добавляют значимости к содержанию текста.
# Пример: Текст: "Машинное обучение, 2024! #AI"
# Без символов: "Машинное обучение"

# Зачем это нужно?
# Чтобы избежать влияния ненужных символов на обучение модели.


# Лемматизация
# Что это?
#
# Приведение слов к их базовой форме (лемме), чтобы сгруппировать разные формы одного слова.
# Пример: Слово: "работает"
# Лемма: "работать"

# Зачем это нужно?
# Чтобы сократить количество уникальных признаков и улучшить обобщающую способность модели.


# Стемминг
# Что это?
#
# Удаление суффиксов и префиксов для приведения слова к его "корню".
# Пример: Слово: "работает"
# Стемминг: "работ"

# Зачем это нужно?
# Уменьшить сложность модели, но с риском потери смысла из-за агрессивной обработки.



# Bag of Words (BoW)
# Что это?
#
# Bag of Words (мешок слов) — это простая техника представления текста в виде числовых данных. Она создает вектор для каждого текста, где каждый элемент вектора соответствует частоте появления слова из словаря.
# Как это работает?
#
# Словарь: Создается список уникальных слов (токенов) из всего набора текстов.
# Векторы: Для каждого текста создается вектор, где:
# Каждая позиция соответствует слову из словаря.
# Значение позиции — частота появления этого слова в тексте.
# Пример: Тексты:
#
# "машинное обучение круто"
# "обучение это интересно"
# Словарь: ["машинное", "обучение", "круто", "это", "интересно"]
# Векторы:
#
# [1, 1, 1, 0, 0]
# [0, 1, 0, 1, 1]
# Плюсы:
#
# Простота реализации.
# Подходит для моделей, работающих с разреженными матрицами.
# Минусы:
#
# Учитывает только частоту, не выделяя значимость слов (например, часто встречающиеся слова вроде "это" могут получить высокий вес).
# Игнорирует порядок слов и контекст.
# Где в вашем коде?
#
# Реализован через CountVectorizer для преобразования текстов в числовые векторы.
# Зачем это нужно?
#
# BoW позволяет использовать текстовые данные для обучения моделей машинного обучения.


# Term Frequency - Inverse Document Frequency (TF-IDF)
# Что это?
#
# TF-IDF — это метод взвешивания слов в тексте, который учитывает не только частоту слова, но и его значимость.
# TF-IDF выделяет слова, важные для конкретного текста, но редко встречающиеся в других текстах.

#
# Учитывает значимость слов в контексте всех документов.
# Снижает вес часто встречающихся слов (например, "это", "и").
# Минусы:
#
# Сложнее, чем BoW.
# Может быть менее эффективным на малых наборах данных.


# Заключение
# Bag of Words: Лучше для простых задач или небольших данных.
# TF-IDF: Идеален для более сложных задач, где важно учитывать значимость слов.


# Для моего проекта можно попробовать заменить CountVectorizer на TfidfVectorizer и сравнить результаты.
# Это поможет понять, какое представление текста лучше подходит для вашей задачи.